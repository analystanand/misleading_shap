# Misleading_shap

The Pitfalls of SHAP Values in Explainable AI

This research article examines the use of Shapley values, specifically SHAP scores, for explaining machine learning model predictions. The authors demonstrate that SHAP scores, while popular, can provide misleading information about feature importance, potentially leading human decision-makers to erroneous conclusions. They propose a formal, logic-based approach to explainability that offers stronger guarantees of rigor than informal methods like SHAP. The study uses decision trees as illustrative examples, highlighting cases where SHAP scores contradict intuitive analyses and rigorous formal explanations. The authors conclude that the foundations of SHAP scores need revision, especially concerning high-stakes applications.

This work is attempt to reproduce the claims made by https://cacm.acm.org/research/explainability-is-not-a-game/
